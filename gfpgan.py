# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fgfZvjZSkFoImob8dW9LBGGVo8Gfg07M
"""

import tensorflow as tf
from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Conv2D, Conv2DTranspose
from tensorflow.keras.layers import BatchNormalization, LeakyReLU
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.datasets import mnist
import numpy as np

class GFP_GAN:
    def __init__(self, img_shape=(28,28,1), z_dim=100, alpha=0.2, lr=0.0002):
        self.img_shape = img_shape
        self.z_dim = z_dim
        self.alpha = alpha
        self.lr = lr
        
        self.discriminator = self.build_discriminator()
        self.generator = self.build_generator()
        
        self.discriminator.compile(loss='binary_crossentropy',
                                   optimizer=Adam(lr=lr),
                                   metrics=['accuracy'])
        
        z = Input(shape=(self.z_dim,))
        img = self.generator(z)
        self.discriminator.trainable = False
        validity = self.discriminator(img)
        self.gfp_gan = Model(z, validity)
        self.gfp_gan.compile(loss=self.gradient_penalty_loss,
                             optimizer=Adam(lr=lr, beta_1=0.5),
                             metrics=['accuracy'])
        
    def gradient_penalty_loss(self, y_true, y_pred, interpolated_samples):
        gradients = tf.gradients(y_pred, interpolated_samples)[0]
        gradients_sqr = tf.square(gradients)
        gradients_sqr_sum = tf.reduce_sum(gradients_sqr, axis=np.arange(1, len(gradients_sqr.shape)))
        gradients_l2_norm = tf.sqrt(gradients_sqr_sum)
        gradient_penalty = tf.reduce_mean(tf.square(gradients_l2_norm - 1))
        return gradient_penalty
    
    def build_discriminator(self):
        input_img = Input(shape=self.img_shape)
        
        x = Conv2D(64, (5,5), strides=(2,2), padding='same')(input_img)
        x = LeakyReLU(alpha=self.alpha)(x)
        x = Conv2D(128, (5,5), strides=(2,2), padding='same')(x)
        x = LeakyReLU(alpha=self.alpha)(x)
        x = Flatten()(x)
        x = Dense(1, activation='sigmoid')(x)
        
        discriminator = Model(input_img, x)
        return discriminator
    
    def build_generator(self):
        input_noise = Input(shape=(self.z_dim,))
        
        x = Dense(7*7*256)(input_noise)
        x = BatchNormalization()(x)
        x = LeakyReLU(alpha=self.alpha)(x)
        x = Reshape((7,7,256))(x)
        x = Conv2DTranspose(128, (5,5), strides=(1,1), padding='same')(x)
        x = BatchNormalization()(x)
        x = LeakyReLU(alpha=self.alpha)(x)
        x = Conv2DTranspose(64, (5,5), strides=(2,2), padding='same')(x)
        x = BatchNormalization()(x)
        x = LeakyReLU(alpha=self.alpha)(x)
        x = Conv2DTranspose(1, (5,5), strides=(2,2), padding='same', activation='tanh')(x)
        
        generator = Model(input_noise, x)
        return generator
    
    def train(self, epochs, batch_size, sample_interval=50):
        (X_train, _), (_, _) = mnist.load_data()
        X_train = X_train / 127.5 - 1.
        X_train = np.expand_dims(X_train, axis=3)
        real = np.ones((batch_size, 1))
        fake = np.zeros((batch_size, 1))
        for epoch in range(epochs):
            idx = np.random.randint(0, X_train.shape[0], batch_size)
            real_imgs = X_train[idx]
            noise = np.random.normal(0, 1, (batch_size, self.z_dim))
            fake_imgs = self.generator.predict(noise)
            d_loss_real = self.discriminator.train_on_batch(real_imgs, real)
            d_loss_fake = self.discriminator.train_on_batch(fake_imgs, fake)
            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)
            noise = np.random.normal(0, 1, (batch_size, self.z_dim))
            g_loss = self.gfp_gan.train_on_batch(noise, real)
            if epoch % sample_interval == 0:
                print("Epoch %d: [D loss: %f, acc.: %.2f%%] [G loss: %f]" %
                      (epoch, d_loss[0], 100*d_loss[1], g_loss))
        
    def generate(self, n_samples):
        noise = np.random.normal(0, 1, (n_samples, self.z_dim))
        return self.generator.predict(noise)


    def save_model(self, model_dir):
        if not os.path.exists(model_dir):
            os.makedirs(model_dir)
        self.generator.save(os.path.join(model_dir, "generator.h5"))
        self.discriminator.save(os.path.join(model_dir, "discriminator.h5"))

    @staticmethod
    def load_model(model_dir):
        generator = load_model(os.path.join(model_dir, "generator.h5"))
        discriminator = load_model(os.path.join(model_dir, "discriminator.h5"))
        gfp_gan = GFP_GAN(generator, discriminator)
        return gfp_gan
    def evaluate_discriminator(self, images, labels):
        return self.discriminator.evaluate(images, labels, verbose=False)
    def save_images(self, n_samples, file_path, image_size=(28, 28), n_cols=10):
        fake_images = self.generate(n_samples)
        fake_images = (fake_images + 1) / 2.0 # Scale from [-1, 1] to [0, 1]
        fig, axs = plt.subplots(n_samples // n_cols, n_cols, figsize=(10, 10))
        cnt = 0
        for i in range(n_samples // n_cols):
            for j in range(n_cols):
                axs[i, j].imshow(fake_images[cnt].reshape(*image_size), cmap='gray')
                axs[i, j].axis('off')
                cnt += 1
        fig.savefig(file_path)
        plt.close()

@tf.function
def train_step(real_images):
    batch_size = tf.shape(real_images)[0]
    random_latent_vectors = tf.random.normal(shape=(batch_size, latent_dim))
    with tf.GradientTape() as tape:
        fake_images = generator(random_latent_vectors, training=True)
        real_output = discriminator(real_images, training=True)
        fake_output = discriminator(fake_images, training=True)
        interpolated_samples = interpolate(real_images, fake_images)
        gradient_penalty = tf__gradient_penalty_loss(discriminator, real_images, fake_images, interpolated_samples)
        discriminator_loss = discriminator_loss_fn(real_output, fake_output, gradient_penalty)
    discriminator_grads = tape.gradient(discriminator_loss, discriminator.trainable_weights)
    discriminator_optimizer.apply_gradients(zip(discriminator_grads, discriminator.trainable_weights))

    random_latent_vectors = tf.random.normal(shape=(batch_size, latent_dim))
    with tf.GradientTape() as tape:
        generated_images = generator(random_latent_vectors, training=True)
        gen_output = discriminator(generated_images, training=True)
        generator_loss = generator_loss_fn(gen_output)
    generator_grads = tape.gradient(generator_loss, generator.trainable_weights)
    generator_optimizer.apply_gradients(zip(generator_grads, generator.trainable_weights))

    return {"d_loss": discriminator_loss, "g_loss": generator_loss}

gan = GFP_GAN()
gan.train(epochs=100, batch_size=128, sample_interval=50)

from keras.models import load_model

generator = load_model('path/to/generator_weights.h5')
discriminator = load_model('path/to/discriminator_weights.h5')

gfp_gan = GFP_GAN(generator, discriminator)

# Create an instance of the GFP-GAN model
gfp_gan = GFP_GAN()

# Load the saved weights
gfp_gan.load_weights('gfp_gan_weights.h5')

# Generate 10 new images
generated_images = gfp_gan.generate(10)

# Visualize the generated images
import matplotlib.pyplot as plt
fig, axs = plt.subplots(1, 10, figsize=(15, 15))
for i in range(10):
    axs[i].imshow(generated_images[i, :, :, 0], cmap='gray')
    axs[i].axis('off')
plt.show()